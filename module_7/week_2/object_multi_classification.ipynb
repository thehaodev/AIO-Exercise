{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRxnXW91wtDt"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"andrewmvd/dog-and-cat-detection\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "-MTOvtGmxQDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from PIL import Image\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.models.resnet import ResNet18_Weights"
      ],
      "metadata": {
        "id": "_I5S_47X3Tr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, annotations_dir, image_dir, transform=None):\n",
        "        self.annotations_dir = annotations_dir\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = self.filter_images_with_multiple_objects()\n",
        "\n",
        "    def filter_images_with_multiple_objects(self):\n",
        "        valid_image_files = []\n",
        "        for f in os.listdir(self.image_dir):\n",
        "            if os.path.isfile(os.path.join(self.image_dir, f)):\n",
        "                img_name = f\n",
        "                annotation_name = os.path.splitext(img_name)[0] + \".xml\"\n",
        "                annotation_path = os.path.join(self.annotations_dir, annotation_name)\n",
        "\n",
        "                if self.count_objects_in_annotation(annotation_path) == 1:\n",
        "                    valid_image_files.append(img_name)\n",
        "        return valid_image_files\n",
        "\n",
        "    def count_objects_in_annotation(self, annotation_path):\n",
        "        try:\n",
        "            tree = ET.parse(annotation_path)\n",
        "            root = tree.getroot()\n",
        "            count = len(root.findall(\"object\"))\n",
        "            return count\n",
        "        except FileNotFoundError:\n",
        "            return 0\n",
        "\n",
        "    def parse_annotation(self, annotation_path):\n",
        "        tree = ET.parse(annotation_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        # Lấy kích thước ảnh để chuẩn hóa\n",
        "        image_width = int(root.find(\"size/width\").text)\n",
        "        image_height = int(root.find(\"size/height\").text)\n",
        "\n",
        "        label = None\n",
        "        bbox = None\n",
        "\n",
        "        for obj in root.findall(\"object\"):\n",
        "            name = obj.find(\"name\").text\n",
        "            if label is None:  # Lấy nhãn đầu tiên\n",
        "                label = name\n",
        "\n",
        "            # Lấy tọa độ bounding box\n",
        "            xmin = int(obj.find(\"bndbox/xmin\").text)\n",
        "            ymin = int(obj.find(\"bndbox/ymin\").text)\n",
        "            xmax = int(obj.find(\"bndbox/xmax\").text)\n",
        "            ymax = int(obj.find(\"bndbox/ymax\").text)\n",
        "\n",
        "            # Chuẩn hóa tọa độ\n",
        "            bbox = [\n",
        "                xmin / image_width,\n",
        "                ymin / image_height,\n",
        "                xmax / image_width,\n",
        "                ymax / image_height,\n",
        "            ]\n",
        "            break  # Chỉ lấy đối tượng đầu tiên\n",
        "\n",
        "        return label, bbox\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_file = self.image_files[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_file)\n",
        "        annotation_name = os.path.splitext(img_file)[0] + \".xml\"\n",
        "        annotation_path = os.path.join(self.annotations_dir, annotation_name)\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        label, bbox = self.parse_annotation(annotation_path)\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # Chuyển nhãn thành số (0 cho mèo, 1 cho chó)\n",
        "        label_num = 0 if label == \"cat\" else 1 if label == \"dog\" else -1\n",
        "\n",
        "        # Chuyển đổi bbox và label thành tensor\n",
        "        bbox_tensor = torch.tensor(bbox, dtype=torch.float32)\n",
        "        label_tensor = torch.tensor(label_num, dtype=torch.float32)\n",
        "\n",
        "        return img, bbox_tensor, label_tensor\n",
        "\n",
        "    def merge_images(self):\n",
        "        idx1 = random.randint(0, len(self.image_files) - 1)\n",
        "        img1_file = self.image_files[idx1]\n",
        "        img1_path = os.path.join(self.image_dir, img1_file)\n",
        "\n",
        "        idx2 = random.randint(0, len(self.image_files) - 1)\n",
        "        img2_file = self.image_files[idx2]\n",
        "        img2_path = os.path.join(self.image_dir, img2_file)\n",
        "\n",
        "        img1 = Image.open(img1_path).convert(\"RGB\")\n",
        "        img2 = Image.open(img2_path).convert(\"RGB\")\n",
        "\n",
        "        # Ghép ảnh\n",
        "        merged_image = Image.new(\"RGB\", (img1.width + img2.width, max(img1.height, img2.height)))\n",
        "        merged_image.paste(img1, (0, 0))\n",
        "        merged_image.paste(img2, (img1.width, 0))\n",
        "\n",
        "        merged_annotations = []\n",
        "        merged_annotations.append({\"bbox\": img1.annotations[0], \"label\": img1.annotations[1]})\n",
        "\n",
        "        # Điều chỉnh tọa độ bbox cho img2 nếu cần\n",
        "        new_bbox = [\n",
        "            img2.annotations[0][0] + img1.width / merged_image.width,\n",
        "            img2.annotations[0][1] / merged_image.height,\n",
        "            img2.annotations[0][2] + img1.width / merged_image.width,\n",
        "            img2.annotations[0][3] / merged_image.height,\n",
        "        ]\n",
        "\n",
        "        merged_annotations.append({\"bbox\": new_bbox, \"label\": img2.annotations[1]})\n",
        "\n",
        "        return merged_image, merged_annotations"
      ],
      "metadata": {
        "id": "sGg_QY1kxVEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thư mục dữ liệu\n",
        "annotations_dir = os.path.join('data', 'annotations')\n",
        "image_dir = os.path.join('data', 'images')\n",
        "\n",
        "# Định nghĩa chuyển đổi\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Tạo một dataset và dataloaders\n",
        "dataset = MyDataset(annotations_dir, image_dir, transform=transform)\n",
        "train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
      ],
      "metadata": {
        "id": "YcPzukW2x_gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleYOLO(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleYOLO, self).__init__()\n",
        "        self.backbone = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Loại bỏ lớp phân loại cuối cùng của ResNet\n",
        "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
        "\n",
        "        # Thêm đầu ra YOLO\n",
        "        self.fcs = nn.Linear(2048, 2 + (4 * self.num_classes))  # 2 là số grid cell\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x có dạng: (batch_size, C, H, W)\n",
        "        features = self.backbone(x)\n",
        "        features = torch.nn.functional.adaptive_avg_pool2d(features, (1, 1))  # (batch_size, 2048, 1, 1)\n",
        "        features = features.view(features.size(0), -1)  # (batch_size, 2048)\n",
        "        features = self.fcs(features)\n",
        "        return features"
      ],
      "metadata": {
        "id": "y4rNaNjVx6I_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Khởi tạo model, criterion và optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "num_classes = 2  # Giả sử hai lớp: chó và mèo\n",
        "class_to_idx = {'dog': 0, 'cat': 1}\n",
        "\n",
        "model = SimpleYOLO(num_classes=num_classes).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "aHQ_9HY6xbz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss(output, targets, device, num_classes):\n",
        "    mse_loss = nn.MSELoss()\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    batch_size = output.shape[0]\n",
        "    total_loss = 0\n",
        "\n",
        "    output = output.view(batch_size, 2, 2, 4 + num_classes)  # Reshape output to (batch_size, grid_y, grid_x, 4 + num_classes)\n",
        "\n",
        "    for i in range(batch_size):  # Iterate through each image in the batch\n",
        "        for j in range(len(targets[i])):  # Iterate through objects in the image\n",
        "            bbox_center_x = (targets[i][j][0] + targets[i][j][2]) / 2\n",
        "            bbox_center_y = (targets[i][j][1] + targets[i][j][3]) / 2\n",
        "            grid_x = int(bbox_center_x) * 2\n",
        "            grid_y = int(bbox_center_y) * 2\n",
        "\n",
        "            # Classification loss for the responsible grid cell\n",
        "            label_one_hot = torch.zeros(num_classes, device=device)\n",
        "            label_one_hot[int(targets[i][j][4])] = 1\n",
        "            classification_loss = ce_loss(output[i], grid_y, grid_x, 4, label_one_hot)\n",
        "\n",
        "            # Regression loss for the responsible grid cell\n",
        "            bbox_target = targets[i][j][:4].to(device)\n",
        "            regression_loss = mse_loss(output[i], grid_y, grid_x, 4, bbox_target)\n",
        "\n",
        "            # No Object Loss for other grid cells\n",
        "            no_obj_loss = 0\n",
        "            for other_grid_y in range(2):\n",
        "                for other_grid_x in range(2):\n",
        "                    if other_grid_y != grid_y or other_grid_x != grid_x:\n",
        "                        no_obj_loss += mse_loss(output[i], other_grid_y, other_grid_x, 4, torch.zeros(4, device=device))\n",
        "\n",
        "            total_loss += classification_loss + regression_loss + no_obj_loss\n",
        "\n",
        "    return total_loss / batch_size  # Average loss over the batch"
      ],
      "metadata": {
        "id": "70lf6vlgyHuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, data_loader, device, num_classes):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in data_loader:\n",
        "            images = images.to(device)\n",
        "            output = model(images)\n",
        "\n",
        "            total_loss = calculate_loss(output, targets, device, num_classes)\n",
        "            running_loss += total_loss.item()\n",
        "\n",
        "            for batch_idx in range(images.shape[0]):\n",
        "                for target in targets[batch_idx]:\n",
        "                    all_predictions.append(output[batch_idx, :, :, :].argmax().item())\n",
        "                    all_targets.append(target[4].item())\n",
        "\n",
        "    val_loss = running_loss / len(data_loader)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    all_predictions = torch.tensor(all_predictions, device=device)\n",
        "    all_targets = torch.tensor(all_targets, device=device)\n",
        "    val_accuracy = (all_predictions == all_targets).float().mean().item()\n",
        "\n",
        "    return val_loss, val_accuracy"
      ],
      "metadata": {
        "id": "O72gh9kZzDCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, optimizer, num_epochs, device, num_classes):\n",
        "    best_val_accuracy = 0.0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for images, targets in train_loader:\n",
        "            images = images.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(images)\n",
        "\n",
        "            total_loss = calculate_loss(output, targets, device, num_classes)\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += total_loss.item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(epoch_loss)\n",
        "\n",
        "        # Validation\n",
        "        val_loss, val_accuracy = evaluate_model(model, val_loader, device, num_classes)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "        # Save the best model\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "\n",
        "    return train_losses, val_losses, train_accuracies, val_accuracies"
      ],
      "metadata": {
        "id": "Ci-Lh1hszTMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model, image_path, transform, device, class_to_idx, threshold=0.5):\n",
        "    model.eval()\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    original_width, original_height = image.size\n",
        "\n",
        "    # Thay đổi kích thước hình ảnh theo yêu cầu đầu vào của mô hình\n",
        "    resized_image = image.resize((448, 448))\n",
        "    resized_width, resized_height = resized_image.size\n",
        "\n",
        "    # Áp dụng các phép biến đổi giống như trong quá trình huấn luyện\n",
        "    transformed_image = transform(resized_image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(transformed_image).view(1, 2, 2, 4 + len(class_to_idx))  # Điều chỉnh cho lưới 2x2\n",
        "\n",
        "    fig, ax = plt.subplots(1)\n",
        "    ax.axis(\"off\")\n",
        "    ax.imshow(resized_image)  # Hiển thị hình ảnh đã thay đổi kích thước\n",
        "\n",
        "    for grid_y in range(2):\n",
        "        for grid_x in range(2):\n",
        "            # Dự đoán lớp và bounding box cho ô lưới hiện tại\n",
        "            class_pred = output[0, grid_y, grid_x, 4:].argmax().item()\n",
        "            bbox = output[0, grid_y, grid_x, :4].tolist()  # Bounding box dự đoán\n",
        "            # Độ tin cậy của lớp dự đoán\n",
        "            confidence = torch.softmax(output[0, grid_y, grid_x, 4:], dim=0)[class_pred]\n",
        "\n",
        "            # Nếu độ tin cậy > threshold\n",
        "            if confidence > threshold:\n",
        "                x_min = bbox[0] * (resized_width / 2) + grid_x * (resized_width / 2)\n",
        "                y_min = bbox[1] * (resized_height / 2) + grid_y * (resized_height / 2)\n",
        "                x_max = bbox[2] * (resized_width / 2) + grid_x * (resized_width / 2)\n",
        "                y_max = bbox[3] * (resized_height / 2) + grid_y * (resized_height / 2)\n",
        "\n",
        "                # Vẽ bounding box và nhãn trên hình ảnh nếu độ tin cậy lớn hơn ngưỡng\n",
        "                rect = patches.Rectangle(\n",
        "                    (x_min, y_min),\n",
        "                    x_max - x_min,\n",
        "                    y_max - y_min,\n",
        "                    linewidth=1,\n",
        "                    edgecolor=\"r\",\n",
        "                    facecolor=\"none\"\n",
        "                )\n",
        "                ax.add_patch(rect)\n",
        "                ax.text(\n",
        "                    x_min,\n",
        "                    y_min,\n",
        "                    f\"{list(class_to_idx.keys())[class_pred]}: {confidence:.2f}\",\n",
        "                    color=\"white\",\n",
        "                    fontsize=12,\n",
        "                    bbox=dict(facecolor=\"red\", alpha=0.5)\n",
        "                )\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Tải mô hình tốt nhất\n",
        "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "\n",
        "# Dự đoán trên một hình ảnh mẫu\n",
        "image_path = os.path.join(\"image_dir\", \"cat.100.jpg\")\n",
        "# Đường dẫn đến hình ảnh bạn muốn kiểm tra\n",
        "image_path = \"/mnt/c/Study/DD Project/good_1.jpg\"\n",
        "inference(model, image_path, transform, device, class_to_idx, threshold=0.5)"
      ],
      "metadata": {
        "id": "p39_JdLn0DDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cacqvsl90F0x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}