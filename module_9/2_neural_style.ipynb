{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gzgGu4IN-MB",
        "outputId": "f222bd7e-ab41-4e46-a4d4-dfac1438883d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1mU--DNhy8pWMTljj7jI3FvJwRYRHwAq5\n",
            "To: /content/content_img.jpg\n",
            "100% 1.15M/1.15M [00:00<00:00, 40.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=13XpLuVuxI6ekdEf5UElKH_IMWrK8wZU1\n",
            "To: /content/style_img2.jpg\n",
            "100% 1.02M/1.02M [00:00<00:00, 132MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1mU--DNhy8pWMTljj7jI3FvJwRYRHwAq5\n",
        "!gdown 13XpLuVuxI6ekdEf5UElKH_IMWrK8wZU1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyZxAhvd6i85"
      },
      "source": [
        "## 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkxOtv6w6i86"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S15oJOFG6i87"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvnMQ2BS6i87"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrjwG_Ta6i87"
      },
      "source": [
        "## 2. Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmI6sEcm6i87"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "imsize = 256\n",
        "\n",
        "\n",
        "img_transforms = transforms.Compose([\n",
        "    transforms.Resize((imsize, imsize)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qn4d8mqo6i88"
      },
      "outputs": [],
      "source": [
        "def image_loader(image_name):\n",
        "    image = Image.open(image_name)\n",
        "    image = img_transforms(image).unsqueeze(0)\n",
        "    return image.to(device, torch.float)\n",
        "\n",
        "style_img = image_loader(\"style_img2.jpg\")\n",
        "content_img = image_loader(\"content_img.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2eN-LC16i88"
      },
      "outputs": [],
      "source": [
        "unloader = transforms.ToPILImage()\n",
        "\n",
        "def imshow(tensor, title=None):\n",
        "    image = tensor.cpu().clone()\n",
        "    image = image.squeeze(0)\n",
        "    image = unloader(image)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "imshow(style_img, title='Style Image')\n",
        "\n",
        "plt.figure()\n",
        "imshow(content_img, title='Content Image')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u9b-jom6i88"
      },
      "source": [
        "## 3. Loss Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_mxXpUQ6i89"
      },
      "source": [
        "### 3.1 Content Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_vTXTmw6i89"
      },
      "outputs": [],
      "source": [
        "content_weight = 1.0\n",
        "ContentLoss = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyyYHCUq6i89"
      },
      "source": [
        "### 3.2 Style Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WFkS14J6i89"
      },
      "outputs": [],
      "source": [
        "def gram_matrix(tensor):\n",
        "    a, b, c, d = tensor.size()\n",
        "    tensor = tensor.view(a * b, c * d)\n",
        "    G = torch.mm(tensor, tensor.t())\n",
        "    return G.div(a * b * c * d)\n",
        "\n",
        "style_weight = 1e6\n",
        "StyleLoss = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZovf7JT6i89"
      },
      "source": [
        "## 3. Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIZ-08Bz6i89",
        "outputId": "3f5222eb-e1a8-4067-f966-54921e11f7be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (1): ReLU(inplace=True)\n",
              "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (3): ReLU(inplace=True)\n",
              "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (6): ReLU(inplace=True)\n",
              "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (8): ReLU(inplace=True)\n",
              "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (11): ReLU(inplace=True)\n",
              "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (13): ReLU(inplace=True)\n",
              "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (15): ReLU(inplace=True)\n",
              "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (17): ReLU(inplace=True)\n",
              "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (20): ReLU(inplace=True)\n",
              "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (22): ReLU(inplace=True)\n",
              "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (24): ReLU(inplace=True)\n",
              "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (26): ReLU(inplace=True)\n",
              "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (29): ReLU(inplace=True)\n",
              "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (31): ReLU(inplace=True)\n",
              "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (33): ReLU(inplace=True)\n",
              "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (35): ReLU(inplace=True)\n",
              "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from torchvision.models import vgg19, VGG19_Weights\n",
        "\n",
        "VGG19_pretrained = vgg19(weights=VGG19_Weights.DEFAULT).features.eval()\n",
        "VGG19_pretrained.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Normalization(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Normalization, self).__init__()\n",
        "        self.mean = torch.tensor(torch.tensor([0.485, 0.456, 0.406]).to(device)).view(-1, 1, 1)\n",
        "        self.std = torch.tensor(torch.tensor([0.229, 0.224, 0.225]).to(device)).view(-1, 1, 1)\n",
        "\n",
        "    def forward(self, img):\n",
        "        return (img - self.mean) / self.std\n",
        "\n",
        "normalization = Normalization().to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xLhurf4Xy17",
        "outputId": "9d926746-feb3-4968-e9ed-8b56454c187e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-74401423e966>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.mean = torch.tensor(torch.tensor([0.485, 0.456, 0.406]).to(device)).view(-1, 1, 1)\n",
            "<ipython-input-11-74401423e966>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.std = torch.tensor(torch.tensor([0.229, 0.224, 0.225]).to(device)).view(-1, 1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4bX3Xw_6i89"
      },
      "outputs": [],
      "source": [
        "content_layers = ['conv_4']\n",
        "style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EFoHggz6i89"
      },
      "outputs": [],
      "source": [
        "def get_features(pretrained_model, image):\n",
        "    layers = {\n",
        "        '0': 'conv_1',\n",
        "        '5': 'conv_2',\n",
        "        '16': 'conv_3',\n",
        "        '25': 'conv_4',\n",
        "        '34': 'conv_5'\n",
        "    }\n",
        "    features = {}\n",
        "    x = image\n",
        "    x = normalization(x)\n",
        "    for name, pretrained_layer in pretrained_model._modules.items():\n",
        "        x = pretrained_layer(x)\n",
        "        if name in layers:\n",
        "            features[layers[name]] = x\n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_MZZ-xlN-MH"
      },
      "outputs": [],
      "source": [
        "def rot_style_features(style_features, style_layers):\n",
        "    final_rot_style_features = {}\n",
        "    for layer in style_layers:\n",
        "        sf = style_features[layer].clone()\n",
        "\n",
        "        rot90 = torch.rot90(sf.clone(), 1, (2, 3))\n",
        "        rot180 = torch.rot90(rot90.clone(), 1, (2, 3))\n",
        "        final_rot = sf + (rot90 - rot180)\n",
        "\n",
        "        final_rot_style_features[layer] = final_rot\n",
        "    return final_rot_style_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfH9P49i6i89"
      },
      "outputs": [],
      "source": [
        "content_features = get_features(VGG19_pretrained, content_img)\n",
        "style_features1 = get_features(VGG19_pretrained, style_img)\n",
        "final_rot_style_features = rot_style_features(style_features1, style_layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VgFr8cm6i89"
      },
      "source": [
        "## 4. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYFF__he6i89"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "target_img1 = content_img.clone().requires_grad_(True).to(device)\n",
        "target_img2 = content_img.clone().requires_grad_(True).to(device)\n",
        "optimizer1 = optim.Adam([target_img1], lr=0.02)\n",
        "optimizer2 = optim.Adam([target_img2], lr=0.02)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trqDgkygN-MI"
      },
      "outputs": [],
      "source": [
        "\n",
        "def style_tranfer_(model, optimizer, target_img,\n",
        "                   content_features, style_features,\n",
        "                   style_layers, content_weight, style_weight):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    with torch.no_grad():\n",
        "        target_img.clamp_(0, 1)\n",
        "    target_features = get_features(model, target_img)\n",
        "\n",
        "    content_loss = ContentLoss(content_features['conv_4'], target_features['conv_4'])\n",
        "\n",
        "    style_loss = 0\n",
        "    for layer in style_layers:\n",
        "        target_gram = gram_matrix(target_features[layer])\n",
        "        style_gram = gram_matrix(style_features[layer])\n",
        "        style_loss += StyleLoss(style_gram, target_gram)\n",
        "\n",
        "    total_loss = content_loss*content_weight + style_loss*style_weight\n",
        "    total_loss.backward(retain_graph=True)\n",
        "    optimizer.step()\n",
        "    return total_loss, content_loss, style_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UllrF3V6i89",
        "outputId": "1fab96d8-c08c-484c-fa38-3ca031f325af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/500] Total loss1: 7.738296 -                 Content loss1: 4.817998 - Style loss1: 0.000003\n",
            "Epoch [100/500] Total loss2: 235.317734 -                 Content loss1: 9.023441 - Style loss2: 0.000226\n",
            "Epoch [200/500] Total loss1: 6.499187 -                 Content loss1: 4.412474 - Style loss1: 0.000002\n",
            "Epoch [200/500] Total loss2: 214.116150 -                 Content loss1: 9.068839 - Style loss2: 0.000205\n",
            "Epoch [300/500] Total loss1: 5.983521 -                 Content loss1: 4.167047 - Style loss1: 0.000002\n",
            "Epoch [300/500] Total loss2: 205.830109 -                 Content loss1: 9.088188 - Style loss2: 0.000197\n",
            "Epoch [400/500] Total loss1: 5.750158 -                 Content loss1: 4.044306 - Style loss1: 0.000002\n",
            "Epoch [400/500] Total loss2: 201.091217 -                 Content loss1: 9.107597 - Style loss2: 0.000192\n",
            "Epoch [500/500] Total loss1: 5.577964 -                 Content loss1: 3.953483 - Style loss1: 0.000002\n",
            "Epoch [500/500] Total loss2: 198.637436 -                 Content loss1: 9.196016 - Style loss2: 0.000189\n"
          ]
        }
      ],
      "source": [
        "STEPS = 500\n",
        "\n",
        "for step in range(STEPS):\n",
        "\n",
        "    total_loss1, content_loss1, style_loss1 = style_tranfer_(VGG19_pretrained, optimizer1, target_img1,\n",
        "                                                           content_features, style_features1,\n",
        "                                                           style_layers, content_weight, style_weight)\n",
        "\n",
        "    total_loss2, content_loss2, style_loss2 = style_tranfer_(VGG19_pretrained, optimizer2, target_img2,\n",
        "                                                           content_features, final_rot_style_features,\n",
        "                                                           style_layers, content_weight, style_weight)\n",
        "\n",
        "    if step % 100 == 99:\n",
        "        print(f\"Epoch [{step+1}/{STEPS}] Total loss1: {total_loss1.item():.6f} - \\\n",
        "                Content loss1: {content_loss1.item():.6f} - Style loss1: {style_loss1.item():.6f}\")\n",
        "        print(f\"Epoch [{step+1}/{STEPS}] Total loss2: {total_loss2.item():.6f} - \\\n",
        "                Content loss1: {content_loss2.item():.6f} - Style loss2: {style_loss2.item():.6f}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        target_img1.clamp_(0, 1)\n",
        "        target_img2.clamp_(0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5r1i6wU6i8-"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.axis('off')\n",
        "\n",
        "imshow(target_img1.detach(), title='Output Image1')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dN0pikYmN-MI"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.axis('off')\n",
        "\n",
        "imshow(target_img2.detach(), title='Output Image2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7B9YLxhSN-MI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}