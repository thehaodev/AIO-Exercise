{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgFQiQq7HFEL"
      },
      "source": [
        "# 1. Install and import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vJbGrAw7Gp2"
      },
      "outputs": [],
      "source": [
        "!pip install numba==0.61.0 torchaudio==2.6.0 librosa==0.10.2.post1 timm -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9tk3k_qb6aI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import IPython.display as ipd\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import timm\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from IPython.display import Audio\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtRGkWO5HFEP"
      },
      "source": [
        "# 2. Download audio dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkA2TXHMHFEQ"
      },
      "outputs": [],
      "source": [
        "!gdown 1u2WzsWUlyZbbPDfXAWXuLRMTwFkT21wa\n",
        "!unzip -q crawled_piano_audio_40_pages.zip -d piano_audio_files_40_pages\n",
        "os.remove(\"crawled_piano_audio_40_pages.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkv47NqYHFEQ"
      },
      "source": [
        "# 3. Define audio data processing functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_get_genres(json_path):\n",
        "    with open(json_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    return data.get('genres', [])\n",
        "\n",
        "json_dir = os.path.join(\"piano_audio_files\", \"crawled_data\")"
      ],
      "metadata": {
        "id": "gymF9SelImTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tN2QzwlHHFEQ"
      },
      "outputs": [],
      "source": [
        "def load_and_resample_audio(file_path, target_sr=22050):\n",
        "    audio, sr = librosa.load(file_path, sr=None)\n",
        "    if sr != target_sr:\n",
        "        audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
        "\n",
        "    return audio, target_sr\n",
        "\n",
        "def audio_to_melspec(audio, sr, n_mels, n_fft=2048, hop_length=512, to_db=False):\n",
        "    spec = librosa.feature.melspectrogram(\n",
        "        y=audio,\n",
        "        sr=sr,\n",
        "        n_fft=n_fft,\n",
        "        hop_length=hop_length,\n",
        "        win_length=None,\n",
        "    )\n",
        "    if to_db:\n",
        "        spec = librosa.power_to_db(spec, ref =np.max)\n",
        "\n",
        "    return spec\n",
        "\n",
        "def normalize_melspec(melspec, norm_range=(0, 1)):\n",
        "    scaler = MinMaxScaler(feature_range=norm_range)\n",
        "    melspec = melspec.T\n",
        "    melspec_normalized = scaler.fit_transform(melspec)\n",
        "    return melspec_normalized.T\n",
        "\n",
        "\n",
        "def denormalize_melspec(melspec_normalized, original_melspec, norm_range=(0, 1)):\n",
        "    scaler = MinMaxScaler(feature_range=norm_range)\n",
        "    melspec = original_melspec.T\n",
        "    scaler.fit(melspec)\n",
        "    melspec_denormalized = scaler.inverse_transform(melspec_normalized.T)\n",
        "    return melspec_denormalized.T\n",
        "\n",
        "\n",
        "def melspec_to_audio(melspec, sr, n_fft=2048, hop_length=512, n_iter=64):\n",
        "    if np.any(melspec < 0):\n",
        "        melspec = librosa.db_to_power(melspec)\n",
        "\n",
        "    audio_reconstructed = librosa.feature.inverse.mel_to_audio(\n",
        "        melspec,\n",
        "        sr=sr,\n",
        "        n_fft=n_fft,\n",
        "        hop_length=hop_length,\n",
        "        win_length=None,\n",
        "        window=\"hann\",\n",
        "        center=True,\n",
        "        pad_mode=\"reflect\",\n",
        "        power=2.0,\n",
        "        n_iter=n_iter\n",
        "    )\n",
        "    return audio_reconstructed\n",
        "\n",
        "def display_audio_files(reconstructed_audio, sr, title=\"\", original_audio=None):\n",
        "    if original_audio is not None:\n",
        "        print(\"Original Audio:\")\n",
        "        ipd.display(ipd.Audio(original_audio, rate=sr))\n",
        "        print(\"Reconstructed Audio (from Mel Spectrogram):\")\n",
        "    else:\n",
        "        print(title)\n",
        "\n",
        "    ipd.display(ipd.Audio(reconstructed_audio, rate=sr))\n",
        "\n",
        "def show_spectrogram(spectrogram, title=\"Mel-Spectrogram\", denormalize=False, is_numpy=False):\n",
        "    if not is_numpy:\n",
        "        spectrogram = spectrogram.squeeze().cpu().numpy()\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    if denormalize:\n",
        "        plt.imshow(spectrogram, aspect=\"auto\", origin=\"lower\", cmap=\"viridis\")\n",
        "    else:\n",
        "        plt.imshow(spectrogram, aspect=\"auto\", origin=\"lower\", cmap=\"viridis\", vmin=0, vmax=1)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Mel Frequency\")\n",
        "    plt.colorbar()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHjfnEwYHFER"
      },
      "outputs": [],
      "source": [
        "audio, target_sr = load_and_resample_audio(os.path.join(\"piano_audio_files\", \"crawled_data\", \"audio\", \"audio_0001.mp3\"))\n",
        "\n",
        "mel_spectrogram = audio_to_melspec(audio, target_sr, n_mels=256)\n",
        "\n",
        "show_spectrogram(mel_spectrogram, title=\"Original Mel-Spectrogram\", is_numpy=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yR4KsL75HFER"
      },
      "outputs": [],
      "source": [
        "json_dir = os.path.join(\"piano_audio_files\", \"crawled_data\")\n",
        "all_genres = []\n",
        "\n",
        "for filename in os.listdir(json_dir):\n",
        "    if filename.endswith('.json'):\n",
        "        json_path = os.path.join(json_dir, filename)\n",
        "        genres = load_and_get_genres(json_path)\n",
        "        all_genres.extend(genres)\n",
        "\n",
        "unique_genres = set(all_genres)\n",
        "max_genres = len(unique_genres)\n",
        "print(f\"Total unique genres: {max_genres}\")\n",
        "print(f\"Unique genres: {unique_genres}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ce2whcTHFER"
      },
      "outputs": [],
      "source": [
        "genres2idx = {genre: idx for idx, genre in enumerate(unique_genres)}\n",
        "idx2genres = {idx: genre for genre, idx in genres2idx.items()}\n",
        "\n",
        "def tokenize(genres):\n",
        "    return [genres2idx[genre] for genre in genres if genre in genres2idx]\n",
        "\n",
        "def detokenize_tolist(tokens):\n",
        "    return [idx2genres[token] for token in tokens if token in idx2genres]\n",
        "\n",
        "def onehot_encode(tokens, max_genres):\n",
        "    onehot = np.zeros(max_genres)\n",
        "    onehot[tokens] = 1\n",
        "    return onehot\n",
        "\n",
        "def onehot_decode(onehot):\n",
        "    return [idx for idx, val in enumerate(onehot) if val == 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC6dMuByHFES"
      },
      "source": [
        "# 4. Create PyTorch DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ti8jjfDnRgMb"
      },
      "outputs": [],
      "source": [
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, data_dir, json_dir, sample_rate, duration, n_mels, n_genres, testset_amount=10):\n",
        "        self.data_dir = data_dir\n",
        "        self.files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith(\".mp3\")]\n",
        "        self.json_dir = json_dir\n",
        "        self.json_files = [os.path.join(json_dir, f) for f in os.listdir(json_dir) if f.endswith(\".json\")]\n",
        "        self.sample_rate = sample_rate\n",
        "        self.duration = duration\n",
        "        self.fixed_length = sample_rate * duration\n",
        "        self.n_genres = n_genres\n",
        "        self.n_mels = n_mels\n",
        "\n",
        "        audios = []\n",
        "        for file_path, json_file_path in tqdm(zip(self.files, self.json_files), desc=f\"Loading audio files in {data_dir}\", unit=\"file\", total=len(self.files)):\n",
        "            audio, sr = load_and_resample_audio(file_path, target_sr=sample_rate)\n",
        "            genres_list = load_and_get_genres(json_file_path)\n",
        "\n",
        "            genres_tokens = tokenize(genres_list)\n",
        "            genres_input = onehot_encode(genres_tokens, n_genres)\n",
        "            genres_input = torch.tensor(genres_input, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "            n_samples = len(audio)\n",
        "            n_segments = n_samples // self.fixed_length\n",
        "\n",
        "            for i in range(n_segments):\n",
        "                start = i * self.fixed_length\n",
        "                end = (i + 1) * self.fixed_length\n",
        "                segment = audio[start:end]\n",
        "                mel_spec = audio_to_melspec(segment, sr, self.n_mels, to_db=True)\n",
        "                mel_spec_norm = normalize_melspec(mel_spec)\n",
        "                mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0)\n",
        "                mel_spec_norm = torch.tensor(mel_spec_norm, dtype=torch.float32).unsqueeze(0)\n",
        "                audios.append((mel_spec_norm, genres_input, mel_spec))\n",
        "\n",
        "        self.audios = audios[:len(audios) - testset_amount]\n",
        "        self.testset = audios[len(audios) - testset_amount:]\n",
        "        print(f\"Loaded {len(self.audios)} audio segments from {len(self.files)} files, each with shape: {self.audios[0][0].shape}, {self.audios[0][1].shape}, duration: {duration} seconds\")\n",
        "        print(f\"Test set: {len(self.testset)} audio segments\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audios)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        mel_spec_part, genres_input, mel_spec = self.audios[idx]\n",
        "\n",
        "        return mel_spec_part, genres_input, mel_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AGD_SUvf4oQ"
      },
      "outputs": [],
      "source": [
        "sample_rate = 22050\n",
        "duration = 3\n",
        "n_mels = 256\n",
        "\n",
        "audio_dir = os.path.join(\"piano_audio_files\", \"crawled_data\", \"audio\")\n",
        "json_dir = os.path.join(\"piano_audio_files\", \"crawled_data\")\n",
        "\n",
        "testset_amount = 32\n",
        "trainset = AudioDataset(audio_dir, json_dir, sample_rate, duration,\n",
        "                        n_mels, max_genres, testset_amount=testset_amount)\n",
        "testset = trainset.testset\n",
        "\n",
        "if len(trainset) == 0:\n",
        "    raise ValueError(f\"No .wav file found in {audio_dir}.\")\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=4)\n",
        "testloader = DataLoader(testset, batch_size=testset_amount, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPtusKU2HFES"
      },
      "outputs": [],
      "source": [
        "audios = trainloader.dataset.audios.copy()\n",
        "\n",
        "frame = audios[0][0].shape[-1]\n",
        "print(audios[0][1])\n",
        "print(frame)\n",
        "\n",
        "for i in range(1):\n",
        "    show_spectrogram(audios[i][0], title=\"Clip Mel-Spectrogram\")\n",
        "    spec_denorm = denormalize_melspec(audios[i][0].numpy().squeeze(), audios[i][2].numpy().squeeze())\n",
        "    show_spectrogram(torch.tensor(spec_denorm), title=\"Denormalized Mel-Spectrogram\", denormalize=True)\n",
        "    audio_reconstructed = melspec_to_audio(spec_denorm, sample_rate)\n",
        "    display_audio_files(audio_reconstructed, sample_rate, title=\"Original Audio after convert to Spectrogram and back to Audio\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wzvew38EHFES"
      },
      "source": [
        "# 5. Create VAE model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shn-2KBNHFES"
      },
      "outputs": [],
      "source": [
        "class CVAE(nn.Module):\n",
        "    def __init__(self, d_model, latent_dim, n_frames, n_mels, n_genres):\n",
        "        super(CVAE, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.latent_dim = latent_dim\n",
        "        self.n_frames = int(np.ceil(n_frames / 2**3))\n",
        "        self.n_mels = int(np.ceil(n_mels / 2**3))\n",
        "        self.n_genres = n_genres\n",
        "        print(self.n_frames, self.n_mels)\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1 + self.n_genres, d_model, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(d_model),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout2d(0.05),\n",
        "\n",
        "            nn.Conv2d(d_model, d_model * 2, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(d_model * 2),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout2d(0.1),\n",
        "\n",
        "            nn.Conv2d(d_model * 2, d_model * 4, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(d_model * 4),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout2d(0.15),\n",
        "\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "        )\n",
        "\n",
        "        # Latent space\n",
        "        self.fc_mu = nn.Linear(d_model * 4, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(d_model * 4, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder_input = nn.Linear(latent_dim + self.n_genres * d_model, d_model * 4)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(d_model * 4, d_model * 2, kernel_size=2, stride=2, padding=0),\n",
        "            nn.BatchNorm2d(d_model * 2),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout2d(0.1),\n",
        "\n",
        "            nn.ConvTranspose2d(d_model * 2, d_model, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(d_model),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout2d(0.05),\n",
        "\n",
        "            nn.ConvTranspose2d(d_model, 1, kernel_size=3, stride=2, padding=1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x, genres_input):\n",
        "        ori_genres_emb = genres_input.view(genres_input.size(0), -1)\n",
        "        genres_emb = ori_genres_emb.unsqueeze(-1).unsqueeze(-1)\n",
        "        genres_emb = genres_emb.expand(-1, -1, x.size(2), x.size(3))\n",
        "        x_genres = torch.cat((x, genres_emb), dim=1)\n",
        "\n",
        "        h = x_genres\n",
        "        shortcuts = []\n",
        "        for block in self.encoder:\n",
        "            h = block(h)\n",
        "            if isinstance(block, nn.SiLU):\n",
        "                shortcuts.append(h)\n",
        "\n",
        "        mu = self.fc_mu(h)\n",
        "        logvar = self.fc_logvar(h)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        z_genres = torch.cat((z, ori_genres_emb), dim=1)\n",
        "        h_dec = self.decoder_input(z_genres)\n",
        "        h_dec = h_dec.view(-1, self.d_model * 4, self.n_frames, self.n_mels)\n",
        "\n",
        "        for block in self.decoder:\n",
        "            if isinstance(block, nn.ConvTranspose2d) and shortcuts:\n",
        "                shortcut = shortcuts.pop()\n",
        "                h_dec = h_dec + shortcut\n",
        "            h_dec = block(h_dec)\n",
        "\n",
        "        recon = h_dec[:, :, :x.size(2), :x.size(3)]\n",
        "        return recon, mu, logvar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxlLo8-7HFET"
      },
      "source": [
        "# 6. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEN2T6pFctW8"
      },
      "outputs": [],
      "source": [
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    recon_loss = nn.functional.mse_loss(recon_x, x, reduction=\"sum\")\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return recon_loss + KLD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiyl7-1gcpXT"
      },
      "outputs": [],
      "source": [
        "def train_vae(model, dataloader, optimizer, scheduler, num_epochs, verbose_interval=50):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for epoch in tqdm(range(num_epochs), desc=\"Training\", unit=\"epoch\"):\n",
        "        train_loss = 0\n",
        "        for batch_idx, (data, genres_input, ori_data) in enumerate(dataloader):\n",
        "            data = data.to(device)\n",
        "            genres_input = genres_input.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            recon, mu, logvar = model(data, genres_input)\n",
        "            loss = loss_function(recon, data, mu, logvar)\n",
        "            loss.backward()\n",
        "            train_loss += loss.item()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "        avg_loss = train_loss / len(dataloader.dataset)\n",
        "        losses.append(avg_loss)\n",
        "        print(f\"Epoch {epoch}/{num_epochs}, Loss: {avg_loss:.4f}, Lr: {scheduler.get_last_lr()[0]}\")\n",
        "        if epoch == 0 or (epoch + 1) % verbose_interval == 0:\n",
        "            data = data[0].detach().cpu()\n",
        "            recon_img = recon[0].detach().cpu()\n",
        "            show_spectrogram(data, title=\"Original Spectrogram\")\n",
        "            show_spectrogram(recon_img, title=\"Reconstructed Spectrogram\")\n",
        "    return mu, logvar, losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8fJSSqNc0XJ"
      },
      "outputs": [],
      "source": [
        "d_model = 64\n",
        "latent_dim = 128\n",
        "lr = 2e-4\n",
        "num_epochs = 100\n",
        "step_size = num_epochs//2\n",
        "verbose_interval = num_epochs//10\n",
        "gamma = 0.5\n",
        "\n",
        "model = CVAE(d_model, latent_dim, n_mels, frame, max_genres).to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "print(f\"Total number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
        "mu, logvar, losses = train_vae(model, trainloader, optimizer, scheduler, num_epochs, verbose_interval=verbose_interval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8Tk_Dd4HFET"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"model_checkpoint123.pth\")\n",
        "\n",
        "model = CVAE(d_model, latent_dim, n_mels, frame, max_genres).to(device)\n",
        "model.load_state_dict(torch.load(\"model_checkpoint123.pth\"))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_c3V4UtHFET"
      },
      "outputs": [],
      "source": [
        "def plot_losses(losses, title=\"Training Loss\", xlabel=\"Epochs\", ylabel=\"Loss\", color='b', grid=True):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(losses, color=color, linewidth=2)\n",
        "    plt.title(title, fontsize=16, fontweight=\"bold\")\n",
        "    plt.xlabel(xlabel, fontsize=14)\n",
        "    plt.ylabel(ylabel, fontsize=14)\n",
        "    plt.xticks(fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "    plt.grid(grid, linestyle=\"--\", alpha=0.6)\n",
        "\n",
        "    min_loss_idx = losses.index(min(losses))\n",
        "    max_loss_idx = losses.index(max(losses))\n",
        "\n",
        "    plt.annotate(f\"Min Loss: {min(losses):.4f}\",\n",
        "                 xy=(min_loss_idx, min(losses)),\n",
        "                 xytext=(min_loss_idx + 1, min(losses) + 0.1),\n",
        "                 arrowprops=dict(arrowstyle=\"->\", color=\"green\"),\n",
        "                 fontsize=12, color='green')\n",
        "\n",
        "    plt.annotate(f\"Max Loss: {max(losses):.4f}\",\n",
        "                 xy=(max_loss_idx, max(losses)),\n",
        "                 xytext=(max_loss_idx + 1, max(losses) + 0.1),\n",
        "                 arrowprops=dict(arrowstyle=\"->\", color=\"red\"),\n",
        "                 fontsize=12, color=\"red\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_losses(losses)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEmKmCX_HFET"
      },
      "source": [
        "# 7. Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIPh8JmcHFEU"
      },
      "outputs": [],
      "source": [
        "def inference(model, testloader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        data, genres_input, ori_data = next(iter(testloader))\n",
        "        data = data.to(device)\n",
        "        genres_input = genres_input.to(device)\n",
        "        recon, _, _ = model(data, genres_input)\n",
        "        return recon, genres_input, ori_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mf5SHNNiHFEU"
      },
      "outputs": [],
      "source": [
        "gen_mels, genres_input, ori_data = inference(model, testloader)\n",
        "recon_audios = []\n",
        "ori_audios = []\n",
        "\n",
        "for i in range(len(gen_mels[:num_samples])):\n",
        "    show_spectrogram(ori_data[i], title=f\"Original Spectrogram {i+1}\", denormalize=True)\n",
        "    ori_reconstructed = melspec_to_audio(ori_data[i].cpu().numpy().squeeze(), sample_rate)\n",
        "    ori_audios.append(ori_reconstructed)\n",
        "\n",
        "    spec_denorm = denormalize_melspec(gen_mels[i].cpu().numpy().squeeze(), ori_data[i].cpu().numpy().squeeze())\n",
        "    show_spectrogram(spec_denorm, title=f\"Reconstructed Spectrogram {i+1}\", denormalize=True, is_numpy=True)\n",
        "    audio_reconstructed = melspec_to_audio(spec_denorm, sample_rate)\n",
        "    recon_audios.append(audio_reconstructed)\n",
        "\n",
        "    display_audio_files(audio_reconstructed, sample_rate, original_audio=ori_reconstructed)\n",
        "    print(\"-\"*100, end=\"\\n\\n\")\n",
        "\n",
        "ori_audios = np.concatenate(ori_audios)\n",
        "display_audio_files(ori_audios, sample_rate, title=\"Connect all original audio\")\n",
        "recon_audios = np.concatenate(recon_audios)\n",
        "display_audio_files(recon_audios, sample_rate, title=\"Connect all reconstructed audio\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW5dn2e2HFEU"
      },
      "source": [
        "# 8. Inference (generative)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdNRO9mMHFEU"
      },
      "outputs": [],
      "source": [
        "def generate(model, dataloader, genres_list, num_samples=5, diff_level=1):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        data, old_genres_input, ori_data = next(iter(dataloader))\n",
        "        data = data.to(device)\n",
        "\n",
        "        genres_tokens = tokenize(genres_list)\n",
        "        genres_input = onehot_encode(genres_tokens, model.n_genres)\n",
        "        genres_input = torch.tensor(genres_input, dtype=torch.long).unsqueeze(0)\n",
        "        genres_input = genres_input.repeat(old_genres_input.shape[0], 1)\n",
        "        genres_input = genres_input.to(device)\n",
        "\n",
        "        recon, mu, logvar = model(data, genres_input)\n",
        "        ori_audios = []\n",
        "        recon_audios = []\n",
        "        for i in range(num_samples):\n",
        "            old_genres_list = detokenize_tolist(onehot_decode(old_genres_input[i].squeeze().tolist()))\n",
        "            show_spectrogram(data[i], title=\"Original Spectrogram with Genres: \" + \", \".join(old_genres_list))\n",
        "            show_spectrogram(recon[i], title=\"Reconstructed Spectrogram with Genres: \" + \", \".join(genres_list))\n",
        "\n",
        "            diff_spectrogram = torch.abs(data[i] - recon[i]) * diff_level\n",
        "            show_spectrogram(diff_spectrogram, title=f\"Difference Spectrogram (|Original - Reconstructed|) * {diff_level}\")\n",
        "            print(\"Loss: \", loss_function(recon[i], data[i], mu, logvar).item())\n",
        "\n",
        "            spec_denorm = denormalize_melspec(recon[i].cpu().numpy().squeeze(), ori_data[i].cpu().numpy().squeeze())\n",
        "            audio_reconstructed = melspec_to_audio(spec_denorm, sr=sample_rate)\n",
        "            ori_audio = melspec_to_audio(ori_data[i].cpu().numpy().squeeze(), sr=sample_rate)\n",
        "\n",
        "            recon_audios.append(audio_reconstructed)\n",
        "            ori_audios.append(ori_audio)\n",
        "\n",
        "            display_audio_files(ori_audio, sample_rate, title=\"Reconstructed Audio with Genres: \" + \", \".join(old_genres_list))\n",
        "            display_audio_files(audio_reconstructed, sample_rate, title=\"Reconstructed Audio with Genres: \" + \", \".join(genres_list))\n",
        "\n",
        "        if num_samples > 1:\n",
        "            print(\"-\"*100, \"Connect all audio\", \"-\"*100)\n",
        "            recon_ori_audios = np.concatenate(ori_audios)\n",
        "            display_audio_files(recon_ori_audios, sample_rate, title=\"Connect all original audio\")\n",
        "            recon_audios = np.concatenate(recon_audios)\n",
        "            display_audio_files(recon_audios, sample_rate, title=\"Connect all reconstructed audio\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "khoina_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}